# Environment Variables for Medical AI Training Project
# Copy this file to .env and fill in your actual values

# =============================================================================
# REQUIRED CONFIGURATION
# =============================================================================

# Hugging Face API Token (required for accessing Llama models)
# Get your token from: https://huggingface.co/settings/tokens
HUGGINGFACE_TOKEN=your_token_here

# =============================================================================
# MODEL CONFIGURATION
# =============================================================================

# Model name from Hugging Face Hub
MODEL_NAME=meta-llama/Llama-2-13b-chat-hf

# Output directory for fine-tuned models
OUTPUT_DIR=./results/llama_finetuned

# =============================================================================
# TRAINING PARAMETERS
# =============================================================================

# Batch size (adjust based on GPU memory)
BATCH_SIZE=1

# Maximum sequence length for tokenization
MAX_LENGTH=512

# Number of training epochs
NUM_EPOCHS=1

# Learning rate for training
LEARNING_RATE=2e-5

# =============================================================================
# DATA PATHS
# =============================================================================

# Path to the KDIGO clinical guidelines PDF
PDF_PATH=./data/KDIGO-2022-Clinical-Practice-Guideline-for-Diabetes-Management-in-CKD.pdf

# Path to heart disease dataset
HEART_DATA_PATH=./data/statlog+heart/heart.dat

# Path to echocardiogram dataset
ECHO_DATA_PATH=./data/echocardiogram/echocardiogram.data

# =============================================================================
# LOGGING CONFIGURATION
# =============================================================================

# Directory for log files
LOG_DIR=./logs

# Logging level (DEBUG, INFO, WARNING, ERROR)
LOG_LEVEL=INFO

# =============================================================================
# OPTIONAL CONFIGURATION
# =============================================================================

# Random seed for reproducibility
RANDOM_SEED=42

# Device for training (auto, cpu, cuda)
DEVICE=auto

# Number of workers for data loading
NUM_WORKERS=4

# Save model checkpoints every N steps
SAVE_STEPS=500

# Maximum number of checkpoints to keep
SAVE_TOTAL_LIMIT=2

# Gradient accumulation steps
GRADIENT_ACCUMULATION_STEPS=4

# Warmup steps for learning rate scheduler
WARMUP_STEPS=100

# =============================================================================
# ADVANCED CONFIGURATION (Optional)
# =============================================================================

# Mixed precision training (fp16, bf16, no)
MIXED_PRECISION=fp16

# Gradient clipping max norm
MAX_GRAD_NORM=1.0

# Weight decay for regularization
WEIGHT_DECAY=0.01

# Dataloader pin memory
PIN_MEMORY=true

# Resume training from checkpoint
RESUME_FROM_CHECKPOINT=

# =============================================================================
# CLOUD/DEPLOYMENT CONFIGURATION (Optional)
# =============================================================================

# AWS S3 bucket for model storage
# AWS_S3_BUCKET=your-bucket-name

# Google Cloud Storage bucket
# GCS_BUCKET=your-bucket-name

# Azure Storage container
# AZURE_CONTAINER=your-container-name

# MLflow tracking URI
# MLFLOW_TRACKING_URI=http://localhost:5000

# Weights & Biases project name
# WANDB_PROJECT=medical-ai-training 